{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for finance\n",
    "---\n",
    "## Lecture 11: Topic and sequence modeling\n",
    "---\n",
    "\n",
    "**Damien Ackerer**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "École Polytechnique Fédérale de Lausanne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "  * web scrapping\n",
    "  * topic modeling\n",
    "      * $n$-grams\n",
    "  * sequence modeling\n",
    "      * word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping\n",
    "\n",
    "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.\n",
    "\n",
    "The procedure is as follows:\n",
    "  * fetch a web page\n",
    "  * extract information from it\n",
    "  \n",
    "The first step is *in general* straightforward and use the HTTP. The second step typically implies parsing, searching, reformatting, etc. the download unstructured content to produce a structured dataset.\n",
    "\n",
    "<img src=\"img/webscrapping-banner.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The U.S. Securities and Exchange Commission (SEC)\n",
    "\n",
    "The SEC is an independent agency of the United States federal government that is responsible for enforcing the federal securities laws, proposing securities rules, and regulating the securities industry.\n",
    "\n",
    "The SEC has a three-part mission: to protect investors; maintain fair, orderly, and efficient markets; and facilitate capital formation.\n",
    "\n",
    "Public companies, funds, and large shareholders must publish and/or notify the SEC of major changes possibly affecting investors. In addition, public companies publish report periodically about their activities.\n",
    "\n",
    "These report can actually be accessed on the SEC website, for example at: https://sec.report/\n",
    "Here are some other interesting reports: https://www.investopedia.com/articles/fundamental-analysis/08/sec-forms.asp\n",
    "\n",
    "Today we work on the N-1A form which is the registration form for open-end management companies, such as mutual funds, hedge funds, and ETFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forms available via RSS\n",
    "\n",
    "First, we collect all the documents available in the RSS feed using `feedparser`.\n",
    "\n",
    "Only a fraction of forms are available this way, but this will be sufficient for our goals. The rest can be accessed via the EDGAR database: https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "## URL of the RSS feed\n",
    "rss_url = \"https://sec.report/Form/N-1A.rss\"\n",
    "\n",
    "## Parse the feed\n",
    "feed = ## TODO\n",
    "\n",
    "## Check if there was some error\n",
    "if feed.status != 200:\n",
    "    print(\"Some connection error\", feed.status)\n",
    "\n",
    "print(\"Number of documents in the RSS feed: \" + str(len(feed.entries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What an entry looks like\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping the webpages\n",
    "\n",
    "We make simple HTTP requests below. If you meed more advance requests, for example with page login, maybe you want to have a look at the ` selenium` libray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = feed.entries[0].links[0].href\n",
    "print(url)\n",
    "\n",
    "## Scrape the page content\n",
    "request = Request(url)\n",
    "html = urlopen(request).read().decode()\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have a problem to download the webpage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://httpbin.org/user-agent')\n",
    "my_agent = r.text\n",
    "\n",
    "print(my_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define user-agent\n",
    "headers = ## TODO\n",
    "\n",
    "## Scrape the page content\n",
    "request = Request(url, headers=headers)\n",
    "html = urlopen(request).read().decode()\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But... is it legal?\n",
    "\n",
    "<img src=\"img/Is-web-scraping-legal-2.jpg\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the webpage\n",
    "\n",
    "We use the `BeautifulSoup` library which offer powerful parsing tools for various documents type.\n",
    "\n",
    "See: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "def extract_paragraph(html, pattern='Principal Investment Strategies', verbose=False):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    pattern = re.compile(pattern)\n",
    "    funds = []\n",
    "    ## Find the tags for bold text\n",
    "    for tag in soup.find_all(re.compile(\"^b$\")):\n",
    "        ## If the text content match the pattern\n",
    "        if pattern.match(tag.text):\n",
    "            if(verbose):\n",
    "                print(\"START TAG:\", tag)\n",
    "            fund_info = \"\"\n",
    "            ## Collect all the text content until it reaches another tag for bold text\n",
    "            for c in tag.next_elements:\n",
    "                if isinstance(c, NavigableString):\n",
    "                    fund_info += c.lower()\n",
    "                    if(verbose):\n",
    "                        print(\"*\" *5 + \"\\n\" + c.lower())\n",
    "                if isinstance(c, Tag) and c.name == \"b\":\n",
    "                    if(verbose):\n",
    "                        print(\"*\" *5 + \"\\n\" + \"TAG:\", c)\n",
    "                    if len(c.text.strip()) > 0:\n",
    "                        if(verbose):\n",
    "                            print(\"END HERE\\n\\n\")\n",
    "                        break\n",
    "            funds.append(fund_info)\n",
    "    return funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(request).read().decode()\n",
    "funds = extract_paragraph(html, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization\n",
    "\n",
    "This is similar to what we was done in week 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "## stopwords\n",
    "cachedStopWords = nltk.corpus.stopwords.words('english')\n",
    "## stemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "def tokenize(text, min_length=2):\n",
    "    \"\"\"\n",
    "    A tokenizer typical used for classification\n",
    "    \"\"\"\n",
    "    \n",
    "    ## remove some characters\n",
    "    chars = ['i.e.', '-', '\\n', '\\xa0', '\"', '(', ')', ';', ',', '. ', '“', '”', '·', ':', '\\t', \"’\"]\n",
    "    ## TODO\n",
    "        \n",
    "    ## remove extra spaces and tokenize\n",
    "    words = re.sub('\\s+', ' ', text).split(\" \")\n",
    "    \n",
    "    ## remove stopwords\n",
    "    words = ## TODO\n",
    "    \n",
    "    ## stem words\n",
    "    tokens = ## TODO\n",
    "    \n",
    "    ## remove any token with anything but letters\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    \n",
    "    ## keep only tokens large than min_length\n",
    "    filtered_tokens = ## TODO\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(funds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put everything together\n",
    "\n",
    "Perform the above operations on all the files available, this may take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def scrape_and_parse(feed, headers={}):\n",
    "    docs = []\n",
    "    texts = []\n",
    "    for entry in feed.entries:\n",
    "        time.sleep(0.1) ## XXX\n",
    "        ## Get the url for the entry\n",
    "        url = entry.links[0].href\n",
    "        print(url)\n",
    "        ## Make the request\n",
    "        request = Request(url, headers=headers)\n",
    "        html = urlopen(request).read().decode()\n",
    "        ## parse the webpage\n",
    "        funds = extract_paragraph(html)\n",
    "        ## tokenize\n",
    "        for f in funds:\n",
    "            texts.append(f)\n",
    "            docs.append(tokenize(f))\n",
    "    return docs, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run\n",
    "docs, texts = scrape_and_parse(feed, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup the files\n",
    "\n",
    "In case you want to skip the above steps next time you work on this notebook.\n",
    "\n",
    "The library `pickle` stores files in binary format, which is efficient both in terms of speed and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "filename = \"sec_n1a_backup.pickle\"\n",
    "\n",
    "## Save\n",
    "# filehandler = open(filename, 'wb') \n",
    "# pickle.dump([docs, texts], filehandler)\n",
    "\n",
    "## Load\n",
    "# filehandler = open(filename, 'rb') \n",
    "# docs, texts = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data checks\n",
    "\n",
    "Let's see what the data looks like. In practice, you should probably spend some more time on this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_tokens = [len(d) for d in docs]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(n_tokens, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_docs = [d for d in docs if len(d) > 800]\n",
    "long_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised topic modeling\n",
    "\n",
    "In week 10, the news articles were annotated and we train multiple algorithms to lean the mapping between words to classes. \n",
    "\n",
    "This week, we have no annotations and we want to group the texts by common and meaningful topics.\n",
    "\n",
    "A popular approach is to use LDA models, as desribed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "This approach has become a standard by itself: http://www.jmlr.org/papers/v3/blei03a.html\n",
    "\n",
    "The intuition is as follows:\n",
    "  * each **topic** is a distribution over words\n",
    "  * each **document** is a mixture of corpus-wide topics\n",
    "  * each **word** is drawn from one of those topics\n",
    "\n",
    "Notes:\n",
    "  * a word can belong to multiple topics\n",
    "  * the vocabulary is fixed\n",
    "  * this is *bag of words* approach.\n",
    "\n",
    "<img src=\"img/lda1.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distribution is a family of continuous multivariate probability distributions with one parameter vector $\\alpha>0$. In dimension $K$ its support is the $K-1$ simplex, that is $\\sum_{i=1}^K x_i = 1$.\n",
    "\n",
    "Below are examples of 3-dim Dirichlet distributions for different $\\alpha$ parameters.\n",
    "\n",
    "<img src=\"img/dirdist.png\" alt=\"drawing\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions and notations:**\n",
    "  * $M$ denotes the number of documents\n",
    "  * $N$ is number of words in a given document (document $i$ has $N_i$ words)\n",
    "  * $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions\n",
    "  * $\\beta$ is the parameter of the Dirichlet prior on the per-topic word distribution\n",
    "  * $\\theta_i$ is the topic distribution for document $i$\n",
    "  * $\\varphi_k$ is the word distribution for topic $k$\n",
    "  * $z_{ij}$ is the topic for the $j$-th word in document $i$\n",
    "  * $w_{ij}$ is the specific word.\n",
    "  * $K$ is the number of topics\n",
    "  \n",
    "**The generative process:**\n",
    "  * Give each topic a words distribution: draw $\\varphi_k \\sim \\operatorname{Dir}(\\beta)$, where $k \\in \\{ 1,\\dots,K \\}$ and $\\beta$ typically is sparse\n",
    "  * Give each document topics: draw $\\theta_i \\sim \\operatorname{Dir}(\\alpha)$, where $i \\in \\{ 1,\\dots,M \\}$ and\n",
    "$\\mathrm{Dir}(\\alpha)$ is a (sparse) Dirichlet distribution\n",
    "  * Give each document words: for each of the word positions $i, j$, where $i \\in \\{ 1,\\dots,M \\}$, and $j \\in \\{ 1,\\dots,N_i \\}$\n",
    "    * Choose a topic $z_{i,j} \\sim\\operatorname{Multinomial}(\\theta_i)$\n",
    "    * Choose a word $w_{i,j} \\sim\\operatorname{Multinomial}( \\varphi_{z_{i,j}})$\n",
    "\n",
    "<img src=\"img/Smoothed_LDA.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "**The learning algorithm** is beyond the scope of this lecture :-) If you are interested: https://arxiv.org/abs/1711.04305\n",
    "\n",
    "**LDA models typically use only the most frequent words!** E.g. 1'000 words for relatively small texts and corpus.\n",
    "\n",
    "More details at: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "## Optional filters for the tokens to keep\n",
    "# dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## documents to bag-of-words\n",
    "bow_corpus = ## TODO\n",
    "\n",
    "pprint(bow_corpus[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(docs[42]))\n",
    "\n",
    "dictionary[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform BOW in TF-IDF\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "pprint(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using raw BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(## TODO, \n",
    "                                       num_topics=## TODO, \n",
    "                                       id2word=## TODO, \n",
    "                                       passes=## TODO, \n",
    "                                       workers=## TODO)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(## TODO, \n",
    "                                             num_topics=## TODO, \n",
    "                                             id2word=## TODO, \n",
    "                                             passes=## TODO, \n",
    "                                             workers=## TODO)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('*' * 5 + '\\n', 'Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 42\n",
    "\n",
    "print(docs[i])\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending BOW methods with $n$-grams\n",
    "\n",
    "An $n$-gram is simply a sequence of $n$ items. Standard BOW are made of *unigrams*, that is $n$-grams of length 1. \n",
    "\n",
    "<img src=\"img/ngrams.jpeg\" alt=\"drawing\" width=\"400\"/>\n",
    "  \n",
    "The motivativation for $n$-grams is that it may be able to extract more precise information than unigrams, including for example negation (e.g.\\ \"not good\").\n",
    "\n",
    "\n",
    "Going back to topic modeling, one may observe that important expressions are lost with unigrams. For examples:\n",
    "  * \"short term\" -> [\"short\", \"term\"]\n",
    "  * \"long term\" -> [\"long\", \"term\"]\n",
    "  * \"high yield\" -> [\"high\", \"yield\"]\n",
    "  * \"interest rate\" -> [\"interest\", \"rate\"]\n",
    "  \n",
    "We now train an LDA model on bags of unigrams and bigrams, the code barely changes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a `Phraser` that will create and select a subset of $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "chars = ['-']\n",
    "\n",
    "documents = [\"the mayor of new york was there, the mayor of new york was there\\n\\n\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "print(sentence_stream)\n",
    "\n",
    "bigram = Phrases(sentence_stream, \n",
    "                 min_count=1, ## ignore all words and bigrams with total collected count lower than this.\n",
    "                 threshold=2, ## higher means fewer sentences\n",
    "                 delimiter=b' ')\n",
    "\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "\n",
    "print(bigram_phraser)\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    tokens_ = bigram_phraser[sent]\n",
    "\n",
    "    print(tokens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_stream = [tokenize(t) for t in texts]\n",
    "\n",
    "bigram = Phrases(sentence_stream, \n",
    "                 min_count=1, ## ignore all words and bigrams with total collected count lower than this.\n",
    "                 threshold=5, ## higher means fewer sentences\n",
    "                 delimiter=b' ')\n",
    "\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "\n",
    "print(bigram_phraser[sentence_stream[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dictionnary of unigrams and bigrams\n",
    "sentence_stream_bigram = [bigram_phraser[sent] for sent in sentence_stream]\n",
    "dictionary = gensim.corpora.Dictionary(sentence_stream_bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BOW corpus\n",
    "## TODO\n",
    "\n",
    "## TFIDF\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train LDA model\n",
    "lda_model = ## TODO\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all variables\n",
    "\n",
    "Clean the environment before moving to the next part. Make sure that you saved everything you wanted to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "[Word embeddings](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795) is a set of language modeling and feature learning techniques in NLP where words/phrases/sentences/paragraphs/documents are mapped to vectors of real numbers (usually of less dimentilnality).\n",
    "\n",
    "The idea about this form of dimentionality reduction is to capture semantic/morphological/contextual/hierarchical/etc information as possible from the original text. While training the models to find the embeddings, several directions could be taken:\n",
    "  * preserving the [morphological structure](https://arxiv.org/pdf/1607.04606.pdf) (subword information, etc.);\n",
    "  * [word context](https://arxiv.org/pdf/1411.2738.pdf) representation;\n",
    "  * [global corpus statistics](https://nlp.stanford.edu/pubs/glove.pdf);\n",
    "  * [word hierarchy](https://arxiv.org/pdf/1705.08039.pdf) as in WordNet;\n",
    "  * [relationship between documents](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html) and the terms they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Glove](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "Global Vectors for Word Representation: https://nlp.stanford.edu/projects/glove\n",
    "* [glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip): Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download).\n",
    "* [glove.840B.300d.zip](http://nlp.stanford.edu/data/glove.840B.300d.zip): Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
    "\n",
    "This approach tries to capture the meaning of one word embedding with the structure of the whole observed corpus.\n",
    "\n",
    "This model is trained on the global co-occurrence counts and uses the word statistics.\n",
    "\n",
    "More details and explations are either in the paper or in [this tutorial](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795).\n",
    "\n",
    "<img src=\"img/w2v.jpeg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## transform GloVe to word2vec format\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "## load the Stanford GloVe model in word2vec format\n",
    "filename = 'glove.6B.100d.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector representation of a given word\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=## TODO, \n",
    "                            negative=## TODO, \n",
    "                            topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distance between two words\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarity between two words\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can train a word2vec model for your specific application easily with existing libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised score learning\n",
    "\n",
    "Given financial news headlines, we aim to build a model that can predict the annotated score as precisely as possible. This score is a measure of sentiment.\n",
    "\n",
    "This application is taken from this workshop: https://bitbucket.org/ssix-project/semeval-2017-task-5-subtask-2/src/master/\n",
    "\n",
    "The modeling part is a simplified version of: https://github.com/apmoore1/semeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Read and parse the news headlines and the sentiment scores\n",
    "    \"\"\"\n",
    "    all_data = json.load(open(file_name, 'r'))\n",
    "    text = []\n",
    "    sentiment = []\n",
    "    company = []\n",
    "    for data in all_data:\n",
    "        text.append(data['title'].lower())\n",
    "        company.append(data['company'].lower())\n",
    "        if 'sentiment' in data:\n",
    "            sentiment.append(data['sentiment'])\n",
    "        elif 'sentiment score' in data:\n",
    "            sentiment.append(data['sentiment score'])\n",
    "    return text, np.asarray(sentiment), company\n",
    "\n",
    "## run\n",
    "train_texts, train_sentiments, train_companies = read_data('Headline_Trainingdata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN models for NLP\n",
    "\n",
    "The typical approach is sequence to *something*. But why using sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to sequence\n",
    "\n",
    "They are typically used for text generation such as language translation and chat bots. There could be some *random generator* between the encoder and the decoder.\n",
    "\n",
    "<img src=\"img/seq2seq.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "<img src=\"img/chatbot.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to class / score\n",
    "\n",
    "Replace the decoder by an output layer in the above architecture.\n",
    "\n",
    "For example:\n",
    "<img src=\"img/seq2vec.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "Recent models include sequence reading and predictions in both directions.\n",
    "\n",
    "<img src=\"img/bidirectional.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and word2vec texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def prepare_data(train_texts, word2vec_model, tokenizer):\n",
    "    \n",
    "    ## max number of tokens for headlines, will be used when creating the model\n",
    "    max_token_length = 0\n",
    "\n",
    "    ## tokenization\n",
    "    train_tokens = []\n",
    "    for text in train_texts:\n",
    "        ## tokenize\n",
    "        tokens = tokenizer(text)\n",
    "        tokens = [token for token in tokens if token.strip()]\n",
    "        ## is it longest sentence?\n",
    "        if len(tokens) > max_token_length:\n",
    "            max_token_length = len(tokens)\n",
    "        ## save tokens\n",
    "        train_tokens.append(tokens) \n",
    "\n",
    "    ## word2vec-ization    \n",
    "    vector_length = model.vector_size\n",
    "    all_vectors = []\n",
    "    for tokens in train_tokens:\n",
    "        vector_format = []\n",
    "        for token in tokens:\n",
    "            ## word2vec\n",
    "            if token in word2vec_model.vocab:\n",
    "                ## word embedding\n",
    "                ## TODO: reshape(1,vector_length)\n",
    "                vector_format.append(None)\n",
    "            else:\n",
    "                ## word not found\n",
    "                ## TODO\n",
    "        while len(vector_format) != max_token_length:\n",
    "            ## padding\n",
    "            ## TODO\n",
    "        ## stack all the vector for this sequence\n",
    "        all_vectors.append(np.vstack(vector_format))\n",
    "    \n",
    "    ## stack all the sequences\n",
    "    return np.asarray(all_vectors), max_token_length\n",
    "\n",
    "## run\n",
    "train_vectors, max_length = prepare_data(train_texts, model, word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors[1,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see reasons why LSTM/GRU may be a better choice than simple RNN for this type of applications?\n",
    "\n",
    "<img src=\"img/.png\" alt=\"drawing\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model builder(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Bidirectional, LSTM\n",
    "\n",
    "def build_lstm_model(max_length, vector_length):\n",
    "    \n",
    "    model = ## TODO\n",
    "    \n",
    "    ## output layer: TODO\n",
    "\n",
    "    ## compile model with loss function and optimizer: TODO\n",
    "\n",
    "    return(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and display model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install graphviz\n",
    "!pip3 install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "\n",
    "lstm_model = build_lstm_model(max_length, 100)\n",
    "\n",
    "plot_model(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = lstm_model.fit(train_vectors, train_sentiments, nb_epoch=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with $k$-fold cross-valisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validate(train_text, train_sentiments, n_folds=5, nb_epoch=25,\n",
    "                   shuffle=True, score_function=mean_absolute_error):\n",
    "    \n",
    "    all_results = []\n",
    "    train_text_array = np.asarray(train_text)\n",
    "    train_sentiments_array = np.asarray(train_sentiments)\n",
    "\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=shuffle)\n",
    "    \n",
    "    max_length = train_text.shape[1]\n",
    "    vector_length = train_text.shape[2]\n",
    "    \n",
    "    for train, test in kfold.split(train_text_array, train_sentiments_array):\n",
    "        \n",
    "        lstm_model = build_lstm_model(max_length, vector_length)\n",
    "        lstm_model.fit(train_text_array[train], train_sentiments_array[train], nb_epoch=nb_epoch)\n",
    "        \n",
    "        predicted_sentiments = lstm_model.predict(train_text_array[test])\n",
    "        result = score_function(predicted_sentiments, train_sentiments_array[test])\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cv = cross_validate(train_vectors, train_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "The dataset that we used if fairly small. A common technique to improve generalization in machine learning is to augment the data. In the current situation, this may work as follows:\n",
    "  * random swap of company names\n",
    "  * synonym replacement of positive and negative words\n",
    "  \n",
    "  \n",
    "More reading on data augmentation for NLP: https://arxiv.org/abs/1901.11196\n",
    "\n",
    "<img src=\"img/nlpeda.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
