{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom TensorFlow estimator\n",
    "\n",
    "The objective of this notebook is to show you how one can implement a custom TF estimator, the PCA is only an illustrative application here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data\n",
    "\n",
    "We generate some random data, feel free use real data if you like (such as stock prices). We center the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 3 # latent factor dim\n",
    "nx = 10 # features dim\n",
    "n = 1000 # number of observations\n",
    "seed = 1 \n",
    "\n",
    "## generate data\n",
    "np.random.seed(seed)\n",
    "z_sample = np.random.randn(n, nz)\n",
    "B_mat = np.random.randn(nx, nz)\n",
    "x_sample = (B_mat @ z_sample.T).T\n",
    "\n",
    "## center data\n",
    "x_sample = x_sample - x_sample.mean(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SVD\n",
    "\n",
    "We compute the exact solution using the SVD decomposition, see [here](https://en.wikipedia.org/wiki/Principal_component_analysis#Singular_value_decomposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD\n",
    "ss, us, vs = tf.svd(x_sample, full_matrices=False, compute_uv=True)\n",
    "# 1st eigenvector\n",
    "w1_svd = vs[:,0].numpy()\n",
    "print(w1_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom estimator\n",
    "\n",
    "We build a model to learn the first coordinate transformation (eigenvector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # input data\n",
    "    #x = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    x = features[\"x\"] \n",
    "    \n",
    "    # number of features\n",
    "    nx = params['n_features']\n",
    "    l1p = params['l1_pen']\n",
    "    lr = params['lr']\n",
    "    \n",
    "    # initial value for w1\n",
    "    w_init = np.array([1.0/nx] * nx).astype(np.float32).reshape(-1,1)\n",
    "    # 1st eigenvector\n",
    "    w1 = tf.compat.v1.get_variable(name=\"w1\", \n",
    "                                   initializer= w_init,\n",
    "                                   constraint=lambda x: tf.clip_by_value(x, -1, 1))\n",
    "       \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        ## xxxx\n",
    "        ## this part is just returning w1 in our simple example here\n",
    "        ## BUT in regression/classification it should return the predictions for 'y'\n",
    "        ## xxxx\n",
    "        predictions = {'w1' : tf.identity(w1)}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # new coordinates of x\n",
    "    z = tf.matmul(x, w1)\n",
    "\n",
    "    # objective = max variance of projected points\n",
    "    l0 = -tf.reduce_mean(tf.pow(z,2))\n",
    "    # regularization = L2 norm of w1 should be equal to one\n",
    "    l1 = l1p * tf.pow(tf.reduce_sum(tf.pow(w1,2)) - 1, 2)\n",
    "    # loss function\n",
    "    loss = l0 + l1\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and train the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "n_step = 500\n",
    "lr = 0.1\n",
    "lambda_1 = 1000.0 ## big penalty needed!\n",
    "\n",
    "# simple data ingestion\n",
    "input_fn = lambda: ({\"x\":tf.convert_to_tensor(x_sample, dtype=tf.float32)}, {})\n",
    "\n",
    "# make TFest\n",
    "my_est = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    params={'n_features' : nx, \n",
    "            'l1_pen' : lambda_1,\n",
    "            'lr' : lr\n",
    "            }\n",
    "    )\n",
    "\n",
    "## train\n",
    "my_est.train(input_fn, steps=n_step)\n",
    "\n",
    "preds = my_est.predict(input_fn=lambda: ({\"x\": None}, {}),\n",
    "                       yield_single_examples=False) ## ouput w1 so no need for any input\n",
    "w1_tfe = next(preds)['w1'].reshape(-1)\n",
    "print(w1_tfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(w, name):\n",
    "    ## display basic info about w\n",
    "    print('*' * 5, '\\n', name, \":\\n\",\n",
    "          '\\tw1 norm = ', np.sum(w**2), \n",
    "          '\\n\\tweights =', w,\n",
    "          '\\n\\tvar x proj =', np.sum((x_sample @ w)**2)/n**2)\n",
    "\n",
    "print_summary(w1_tfe, 'PCA TFE')    \n",
    "print_summary(w1_svd, 'PCA SVD')\n",
    "\n",
    "# Sign of axes is arbitrary !!!\n",
    "if np.sign(w1_svd[0]) != np.sign(w1_tfe[0]):\n",
    "    w1_tfe = - w1_tfe\n",
    "\n",
    "x = np.arange(nx)  # the label locations\n",
    "width = 0.45  # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, w1_tfe, width, label='TFE')\n",
    "rects2 = ax.bar(x + width/2, w1_svd, width, label='SVD')\n",
    "ax.set_xlabel('coordinate')\n",
    "ax.set_ylabel('w1 values')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* return the principal components in addition to $w1$ in the prediction step.\n",
    "* how would change the above to learn multiple eigenvectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
