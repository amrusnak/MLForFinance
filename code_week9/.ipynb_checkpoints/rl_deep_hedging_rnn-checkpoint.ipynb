{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep hedging\n",
    "\n",
    "We build a framework to hedge options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBMOptionEnv:\n",
    "    \"\"\"\n",
    "    Environment with:\n",
    "    - GMB stock price\n",
    "    - one call option\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tau=1/4, K=100., mu=0.0, sig=0.2, trans_cost=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the environment.\n",
    "        \"\"\"\n",
    "        ## price dynamics and option\n",
    "        self.tau = tau    ## time to maturity\n",
    "        self.K = K        ## strike price\n",
    "        self.sig = sig    ## volatility parameter\n",
    "        self.mu = mu      ## real-world drift\n",
    "        self.d = 0.0      ## dividend yield\n",
    "        self.r = 0.0      ## risk-free rate\n",
    "        self.S0 = 100.    ## initial underlying price\n",
    "        self.dt = 1./52 ## time increment\n",
    "        self.n = np.int(self.tau / self.dt) ## total number of steps\n",
    "        self.ttms = np.linspace(tau, 0, self.n+1)\n",
    "        ## frictions\n",
    "        self.trans_cost = trans_cost\n",
    "        \n",
    "    def _payoff(self, S):\n",
    "        \"\"\"\n",
    "        Option payoff function\n",
    "        \"\"\"\n",
    "        return max(0., S-self.K)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the episode\n",
    "        Generate the entire price time series (as we assume not price impact)\n",
    "        \"\"\"\n",
    "        dW = np.random.randn(self.n)*np.sqrt(self.dt)          ## innovations\n",
    "        lr = (self.mu - 0.5*self.sig**2)*self.dt + self.sig*dW ## log-returns\n",
    "        self.prices = self.S0*np.exp(np.cumsum(lr)) ## stock prices\n",
    "        self.prices = np.insert(self.prices, 0, self.S0)\n",
    "        self.istep = 0                              ## current step\n",
    "        self.pi = 0.                                ## current number of shares in the risky-asset\n",
    "        self.Si = self.S0                           ## current price\n",
    "        self.ttm = (self.n - self.istep)*self.dt ## time to maturity\n",
    "        return np.array([self.ttm, self.Si, self.pi], dtype=float)\n",
    "        \n",
    "    def step(self, pi):\n",
    "        \"\"\"\n",
    "        Invest pi in the stock, move to the next time step and compute the P&L, proportional transaction costs\n",
    "        \"\"\"\n",
    "        ## compute transaction costs\n",
    "        Si = self.Si\n",
    "        ##-----\n",
    "        ## TODO\n",
    "        ##-----\n",
    "        costs = None\n",
    "        ## get new step and new price\n",
    "        self.istep += 1\n",
    "        Sii = self.prices[self.istep]\n",
    "        ## compute P&L\n",
    "        ##-----\n",
    "        ## TODO\n",
    "        ##-----\n",
    "        pnl = None\n",
    "        if self.istep == self.n:\n",
    "            done = True\n",
    "            pnl += - self._payoff(Sii)\n",
    "        else:\n",
    "            done = False\n",
    "        ## update current state\n",
    "        self.pi = pi\n",
    "        self.Si = Sii\n",
    "        self.ttm = (self.n - self.istep)*self.dt\n",
    "        state_next = np.array([self.ttm, self.Si, self.pi], dtype=float)\n",
    "        ## message\n",
    "        info = \"\"\n",
    "        ## compute reward\n",
    "        reward = pnl - costs\n",
    "        return state_next, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simulate some price trajectories\n",
    "env = GBMOptionEnv()\n",
    "plt.figure()\n",
    "np.random.seed(1)\n",
    "for i in range(10):\n",
    "    env.reset()\n",
    "    plt.plot(env.prices)\n",
    "plt.title(\"GBM paths\")\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-Scholes-Merton formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def BSM_d1(S, tau, r, d, sigma, K):\n",
    "    return (np.log(S/K) + (r-d+sigma**2/2)*tau) / (sigma*np.sqrt(tau))\n",
    "\n",
    "def BSM_price(S, tau, r, d, sigma, K):\n",
    "    Phi = stats.norm(loc=0, scale=1).cdf\n",
    "    d1 = BSM_d1(S, tau, r, d, sigma, K)\n",
    "    d2 = d1 - sigma*np.sqrt(tau)\n",
    "    return S*np.exp(-d*tau)*Phi(d1) - K*np.exp(-r*tau)*Phi(d2)\n",
    "\n",
    "def BSM_delta(S, tau, r, d, sigma, K):\n",
    "    d1 = BSM_d1(S, tau, r, d, sigma, K)\n",
    "    Phi = stats.norm(loc=0, scale=1).cdf\n",
    "    return np.exp(-d*tau)*Phi(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta agent\n",
    "\n",
    "The agent uses the BSM Delta to hedge the call option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaAgent:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initialize the agent\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Hedge the option using the Black-Scholes Delta\n",
    "        \"\"\"\n",
    "        ttm, S, _ = state\n",
    "        delta = BSM_delta(S, ttm, r=self.env.r, d=self.env.d, sigma=self.env.sig, K=self.env.K)\n",
    "        return delta\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        No training!\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test(self, number_of_tests):\n",
    "        rewards = []\n",
    "        for _ in range(number_of_tests):\n",
    "            s = self.env.reset()\n",
    "            total_reward = 0.\n",
    "            for t in range(self.env.n):\n",
    "                a = self.act(s)\n",
    "                s, reward, done, info = self.env.step(a)\n",
    "                total_reward += reward\n",
    "            rewards.append(total_reward)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards):\n",
    "    plt.figure()\n",
    "    plt.hist(rewards)\n",
    "    plt.title(\"Option replication P&Ls (without initial premium)\")\n",
    "    plt.xlabel(\"P&L\")\n",
    "    plt.xlabel(\"# of observations\")\n",
    "    plt.axvline(x=-price, color=\"red\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GBMOptionEnv(trans_cost = 0.0)\n",
    "delta_agent = DeltaAgent(env)\n",
    "n_test = 200\n",
    "rewards_delta = delta_agent.test(n_test)\n",
    "\n",
    "price = BSM_price(env.S0, env.tau, env.r, env.d, env.sig, env.K)\n",
    "\n",
    "plot_rewards(rewards_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - CVaR\n",
    "\n",
    "The agent tries to maximize its $\\alpha$-CVaR while following a RNN policy $\\pi_\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import progressbar as pgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCVaRAgent:\n",
    "    \n",
    "    def __init__(self, env, batch_size, alpha=0.5, gamma=1.0, lr=0.01, \n",
    "                 nodes=[50, 50, 1], \n",
    "                 activations=[\"sigmoid\", \"sigmoid\", \"sigmoid\"]):\n",
    "        \"\"\"\n",
    "        Initialize the agent\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.nodes = nodes\n",
    "        self.activations = activations\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        ## Prepare TF model\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        self.model = self._build_model()\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        RNN model that aim to maximize the alpha-CVaR of the portfolio P&L\n",
    "        \"\"\"\n",
    "        ## input\n",
    "        self.states = tf.placeholder(tf.float32, [self.env.n+1, self.batch_size, 2]) ## only two features here!!!\n",
    "        ## price at maturity\n",
    "        Smat = self.states[-1,:,0]\n",
    "        ## price increments\n",
    "        dS = self.states[1:, :, 0] - self.states[0:-1, :, 0]\n",
    "        \n",
    "        ## prepare states for RNN, remove the last time\n",
    "        ## unstack as input to \"static_rnn\" should be a list of [batch_size, features_size]\n",
    "        states_t = tf.unstack(self.states[:-1, :,:], axis=0) \n",
    "        \n",
    "        ## RNN model\n",
    "        ##-----\n",
    "        ## TODO: in tensorflow\n",
    "        ##-----\n",
    "        cells = [tf.contrib.rnn.BasicRNNCell(self.nodes[i], activation=self.activations[i]) for i in range(len(self.nodes))]\n",
    "        rnn = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        self.strategy, state = tf.nn.static_rnn(rnn, \n",
    "                                                states_t, \n",
    "                                                initial_state=rnn.zero_state(self.batch_size, tf.float32), \n",
    "                                                dtype=tf.float32)\n",
    "        self.strategy = tf.reshape(self.strategy, (self.env.n, self.batch_size))\n",
    "        \n",
    "        ## option payoff at maturity\n",
    "        self.option = tf.maximum(Smat - self.env.K, 0)\n",
    "        \n",
    "        ## daily and final P&L\n",
    "        self.pnl_by_step = dS*self.strategy\n",
    "        self.pnl = tf.reduce_sum(self.pnl_by_step, axis=0) - self.option\n",
    "        \n",
    "        ## compute the CVaR for the confidence level alpha\n",
    "        ## 1. get the largest losses\n",
    "        neg_losses, idx = tf.nn.top_k(-self.pnl, int((1-self.alpha)*self.batch_size))\n",
    "        ## 2. compute their mean\n",
    "        CVaR = tf.reduce_mean(neg_losses)\n",
    "        \n",
    "        ## train op\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(CVaR)\n",
    "    \n",
    "    ## XXX: not applicable here as all hedges are generated by the same model\n",
    "    #def act(self, state):\n",
    "    #    return self.estimator_policy.predict(state, self.sess)\n",
    "    \n",
    "    def _generate_one_batch(self):\n",
    "        states = []\n",
    "        for i in range(self.batch_size):\n",
    "            self.env.reset()\n",
    "            features = np.vstack([self.env.prices, self.env.ttms]).T.reshape(-1,1,2)\n",
    "            states.append(features)\n",
    "        states = np.concatenate(states, axis=1)\n",
    "        return(states)\n",
    "    \n",
    "    def _run_on_batches(self, number_of_episodes, train=True):\n",
    "        num_batches = int(np.ceil(number_of_episodes/self.batch_size))\n",
    "        rewards = []\n",
    "        ## visual for the current episode\n",
    "        bar = pgb.ProgressBar(maxval=num_batches, \n",
    "                              widgets=[pgb.Bar('=', '[', ']'), \n",
    "                                       ' ', pgb.Percentage()])\n",
    "        bar.start()\n",
    "        for e in range(num_batches):\n",
    "            ## pre-processing\n",
    "            states = self._generate_one_batch()\n",
    "            \n",
    "            if train:\n",
    "                _, pnl = self.sess.run([self.train_op, self.pnl], {self.states: states})\n",
    "            else:\n",
    "                 pnl = self.sess.run(self.pnl, {self.states: states})\n",
    "            rewards.append(pnl)\n",
    "            \n",
    "            ## update bar\n",
    "            bar.update(e + 1)\n",
    "            \n",
    "        rewards = np.array(rewards).reshape(-1)   \n",
    "        return rewards\n",
    "        \n",
    "    def train(self, number_of_episodes):\n",
    "        return self._run_on_batches(number_of_episodes, True)\n",
    "    \n",
    "    def test(self, number_of_episodes):\n",
    "        return self._run_on_batches(number_of_episodes, False)\n",
    "    \n",
    "    def get_strategy_batch_sample(self):\n",
    "        states = self._generate_one_batch()\n",
    "        strat, pnl, pnl_by_step= self.sess.run([self.strategy, self.pnl, self.pnl_by_step], {self.states: states})\n",
    "        return strat, pnl, pnl_by_step, states\n",
    "    \n",
    "    def get_deltas_sample(self):\n",
    "        prices = np.linspace(75, 125, self.batch_size)\n",
    "        ttms = self.env.ttms\n",
    "        states = []\n",
    "        for i in range(self.batch_size):\n",
    "            features = np.vstack([np.ones(self.env.n+1)*prices[i], ttms]).T.reshape(-1,1,2)\n",
    "            states.append(features)\n",
    "        states = np.concatenate(states, axis=1)\n",
    "        strat, pnl = self.sess.run([self.strategy, self.pnl], {self.states: states})\n",
    "        return strat, pnl, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = GBMOptionEnv()\n",
    "agent = RNNCVaRAgent(env=env, batch_size=int(1e4))\n",
    "rewards_rnn = agent.train(number_of_episodes=int(2e6))\n",
    "plot_rewards(rewards_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to train longer sometimes\n",
    "#rewards_rnn = agent.train(number_of_episodes=int(1e6))\n",
    "#plot_rewards(rewards_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat, pnl, pnl_by_step, states = agent.get_strategy_batch_sample()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(states[:,0,0]) ## prices\n",
    "plt.title(\"underlying price\")\n",
    "plt.subplot(1,3,2)\n",
    "strat_bs = [BSM_delta(states[i,0,0], states[i,0,1], env.r, env.d, env.sig, env.K) for i in range(env.n)]\n",
    "plt.plot(strat[:,0])    ## delta\n",
    "plt.plot(strat_bs)    ## BSM delta\n",
    "plt.title(\"number of shares\")\n",
    "plt.legend([\"deep hedge\",\"BS Delta hedge\"])\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(np.cumsum(pnl_by_step[:,0]))\n",
    "pnl_by_step_bsm = (states[1:,0,0] - states[:-1,0,0])*strat_bs\n",
    "plt.plot(np.cumsum(pnl_by_step_bsm))\n",
    "plt.title(\"hedge cumulative P&L\")\n",
    "plt.legend([\"deep hedge\",\"BS Delta hedge\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_rnn_test = agent.test(1000)\n",
    "plot_rewards(rewards_rnn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat, pnl, prices = agent.get_deltas_sample()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Deep hedge\")\n",
    "plt.xlabel(\"share price\")\n",
    "plt.ylabel(\"Delta\")\n",
    "for i in range(agent.env.n):\n",
    "    plt.plot(prices, strat[i,:])\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Black-Scholes\")\n",
    "plt.xlabel(\"share price\")\n",
    "plt.ylabel(\"Delta\")\n",
    "prices = np.linspace(75,125,100)\n",
    "for i in range(agent.env.n):\n",
    "    delta = [BSM_delta(p, env.ttms[i], env.r, env.d, env.sig, env.K) for p in prices]\n",
    "    plt.plot(prices, delta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
