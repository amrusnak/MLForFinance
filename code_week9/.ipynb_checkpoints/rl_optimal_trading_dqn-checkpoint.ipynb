{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal allocation revisited with a deep q-network (DQN)\n",
    "\n",
    "In this notebook, we revisit the example of week 8 with the difference that we will implement a deep Q-learning algorithm. \n",
    "\n",
    "The ANN will take as input the state (last action, current price) and the next possible action.\n",
    "\n",
    "Here is the setup:\n",
    "* action: short/neutral/long the asset with a total asset exposure between $-1$ and $1$\n",
    "* state: the asset price (on a finite grid) & the current position \n",
    "* environment: generate new prices and transaction costs\n",
    "\n",
    "Remark: we do not keep track of the accumulated P&L, the objective is only to investigate if the algorithm can learn optimal decisions.\n",
    "\n",
    "The $Q$-table is contructed as follows:\n",
    "\n",
    "| state (prev. action \\& price) / action        | $a_0=0$      | $a_1=1$   | $\\cdots$   | $a_m=m$      |\n",
    "| :------------------ | :----------: | -----------: | -----------: | -----------: | \n",
    "| $s_0=(0,p_{\\min})$       | $Q(s_0,a_0)$ | $Q(s_0,a_1)$ | $\\cdots$  | $Q(s_0,a_m)$ | \n",
    "| $s_1=(0,p_{\\min} + \\Delta)$       | $Q(s_1,a_0)$ | $Q(s_1,a_1)$ | $\\cdots$  | $Q(s_1,a_m)$ | \n",
    "| $s_2=(0,p_{\\min} + 2\\Delta)$       | $Q(s_2,a_0)$ | $Q(s_2,a_1)$ | $\\cdots$  | $Q(s_2,a_m)$ | \n",
    "| $\\vdots$            | $\\vdots$     | $\\vdots$     | | $\\vdots$     | \n",
    "| $s_{n-1}=(m,p_{\\max}-\\Delta)$  | $Q(s_{n-1},a_0)$ | $Q(s_{n-1},a_1)$ | $\\cdots$  | $Q(s_{n-1},a_m)$ | \n",
    "| $s_n=(m,p_{\\max})$       | $Q(s_n,a_0)$ | $Q(s_n,a_1)$ | $\\cdots$  | $Q(s_n,a_m)$ | \n",
    "\n",
    "where $\\Delta$ is the price increment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a class defining the behavior of the environment. Similar logic as the `gym` environment from *OpenAI*, see https://gym.openai.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myTradingEnvV2:\n",
    "    \n",
    "    def __init__(self, timesteps_per_episode):\n",
    "        \"\"\"\n",
    "        Initialiaze the environement\n",
    "        \"\"\"\n",
    "        self.timesteps_per_episode = timesteps_per_episode\n",
    "        self.max_price = 200. ## max price\n",
    "        ##-----\n",
    "        ## TODO\n",
    "        ##-----\n",
    "        self.mult_price = 10 ## price increment\n",
    "        self.n_prices = int(self.max_price / self.mult_price + 1)\n",
    "        ##-----\n",
    "        ## TODO\n",
    "        ##-----\n",
    "        self.n_actions = 5 ## number of possible actions\n",
    "        self.n_states = self.n_prices * self.n_actions ## state = (# of prices) x (# of actions)\n",
    "        self.weights = np.linspace(-1, 1, self.n_actions) ## position percentage\n",
    "         \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Returns the row index of the current state\n",
    "        \"\"\"\n",
    "        price = self.prices[self.last_step]\n",
    "        action = self.last_action\n",
    "        idx = int(price/self.mult_price + action * self.n_prices)\n",
    "        return(idx)\n",
    "    \n",
    "    def get_price_action(self, state):\n",
    "        \"\"\"\n",
    "        Return the state values (current price, previous action) given a state row index\n",
    "        \"\"\"\n",
    "        price = (state % self.n_prices) * self.mult_price\n",
    "        action = state // self.n_prices\n",
    "        return price, action\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the episode, random initial state values\n",
    "        \"\"\"\n",
    "        self.prices = np.zeros(self.timesteps_per_episode)\n",
    "        self.actions = np.zeros(self.timesteps_per_episode)\n",
    "        self.rewards = np.zeros(self.timesteps_per_episode)\n",
    "        ##---------\n",
    "        ## NB: random initial state helps a lot!\n",
    "        ##---------\n",
    "        self.prices[0] = self.mult_price*np.random.randint(self.max_price/self.mult_price + 1) ## random inital price\n",
    "        self.last_action = np.random.randint(self.n_actions) ## random initial position\n",
    "        ##\n",
    "        self.last_step = 0\n",
    "        self.current_state = self.get_state()\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, a):\n",
    "        \"\"\"\n",
    "        Generate the new state and the reward given an action *a*\n",
    "        -----\n",
    "        implemented:\n",
    "        -> mean-reverting process\n",
    "        -----\n",
    "        NOT implemented:\n",
    "        -> reward function\n",
    "        -> transaction cost (later)\n",
    "        -> borrowing cost (optional)\n",
    "        -> price impact (optional)\n",
    "        -> agent risk-aversion (optional)\n",
    "        \"\"\"\n",
    "        ## get previous values\n",
    "        i = self.last_step\n",
    "        x = self.prices[i]\n",
    "        \n",
    "        ## move to next step\n",
    "        if 0 < x < self.max_price:\n",
    "            birth = np.random.rand() <= 0.5 + 0.1*(1. - x / self.max_price)\n",
    "            death = np.random.rand() <= 0.5 + 0.1*(x / self.max_price)\n",
    "            # We update the population size\n",
    "            x_new = x + self.mult_price*birth - self.mult_price*death\n",
    "        else:\n",
    "            ## repulsive boundaries\n",
    "            if x == 0:\n",
    "                x_new = self.mult_price\n",
    "            else:\n",
    "                x_new = self.max_price - self.mult_price\n",
    "        \n",
    "        ##----------\n",
    "        ## TODO: implement reward\n",
    "        ##----------\n",
    "        reward = None\n",
    "        \n",
    "        ## save action\n",
    "        self.actions[i] = a\n",
    "        self.last_action = a\n",
    "        \n",
    "        ## update step\n",
    "        i += 1 \n",
    "        self.last_step = i\n",
    "        ## save price\n",
    "        self.prices[i] = x_new\n",
    "        ## save reward\n",
    "        self.rewards[i] = reward\n",
    "        \n",
    "        ## update current state\n",
    "        s_ = self.get_state()\n",
    "        self.current_state = s_\n",
    "        \n",
    "        ## output some useful information\n",
    "        info = \"\"\n",
    "        \n",
    "        ## end of the time series?\n",
    "        if i == self.timesteps_per_episode - 1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        return s_, reward, done, info\n",
    "    \n",
    "    def simulate(self):\n",
    "        \"\"\"\n",
    "        FOR ILLUSTRATION ONLY\n",
    "        Simulates a price trajectory, assuming no position\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        for t in range(self.timesteps_per_episode - 1):\n",
    "            self.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = myTradingEnvV2(250) ## create environment\n",
    "env.simulate() ## simulate some trajectory\n",
    "plt.plot(env.prices, lw=2)\n",
    "plt.title(\"Price trajectory example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning agent\n",
    "\n",
    "We construct the agent:\n",
    "* its action-value function (ANN)\n",
    "* its decision policy ($\\epsilon$-greedy)\n",
    "* its training routine (simple gradient-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from collections import deque\n",
    "import progressbar as pgb\n",
    "import random as rnd\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env, optimizer, gamma, epsilon):\n",
    "        \"\"\"\n",
    "        Initialize the agent\n",
    "        \"\"\"\n",
    "        \n",
    "        ## initialize atributes\n",
    "        self.env = env\n",
    "        self.n_states = env.n_states\n",
    "        self.n_actions = env.n_actions\n",
    "        self._optimizer = optimizer\n",
    "        \n",
    "        ## initialize discount factor and exploration rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        ## build networks\n",
    "        self.model = self._build_compile_model()\n",
    "    \n",
    "    def _build_compile_model(self):\n",
    "        \"\"\"\n",
    "        Build the ANN that approximate the Q-table\n",
    "        \"\"\"\n",
    "        ##----------\n",
    "        ## TODO\n",
    "        ##----------\n",
    "        ## inputs:\n",
    "        ##    current state:\n",
    "        ##    - previous action\n",
    "        ##    - current price\n",
    "        ##    action:\n",
    "        ##    - next action\n",
    "        ## outputs:\n",
    "        ##    Q-value\n",
    "        ##----------\n",
    "        ## create a Sequential Keras model\n",
    "        model = None\n",
    "        \n",
    "        ## compile the model\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        epsilon-greedy algorithm\n",
    "        -----\n",
    "        Returns a random action with probability \"epsilon\",\n",
    "        or returns the action that maximizes the Q-table for the row \"state\"\n",
    "        \"\"\"\n",
    "        ##----------\n",
    "        ## TODO\n",
    "        ##----------\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        ##----------\n",
    "        ## TODO\n",
    "        ##----------\n",
    "        price, action = self.env.get_price_action(state)\n",
    "        q_values = self.model.predict(np.vstack([[price, action, i] for i in range(env.n_actions)]))\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def max_q_value(self, next_state):\n",
    "        \"\"\"\n",
    "        Returns the largest q-value from the row Q[s,:] given a state \"s\" (= row index)\n",
    "        \"\"\"\n",
    "        next_action = self.act(next_state, 0.0)\n",
    "        price, past_action = self.env.get_price_action(next_state)\n",
    "        q_value = self.model.predict(np.array([[price, past_action, next_action]]))\n",
    "        return q_value\n",
    "    \n",
    "    def train(self, num_of_episodes):\n",
    "        \"\"\"\n",
    "        Train the DQN over multiple episodes\n",
    "        \"\"\"\n",
    "        timesteps_per_episode = self.env.timesteps_per_episode\n",
    "        rewards = []\n",
    "        \n",
    "        for e in range(0, num_of_episodes):\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            # Reset the enviroment\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, 1])\n",
    "\n",
    "            # initialize variables\n",
    "            reward = 0\n",
    "            terminated = False\n",
    "\n",
    "            ## visual for the current episode\n",
    "            bar = pgb.ProgressBar(maxval=timesteps_per_episode/10, \n",
    "                                  widgets=[pgb.Bar('=', '[', ']'), \n",
    "                                           ' ', pgb.Percentage()])\n",
    "            bar.start()\n",
    "\n",
    "            for timestep in range(timesteps_per_episode):\n",
    "                \n",
    "                ## generate action \n",
    "                action = self.act(state, self.epsilon)\n",
    "\n",
    "                ## environment change    \n",
    "                next_state, reward, terminated, info = self.env.step(action) \n",
    "                next_state = np.reshape(next_state, [1, 1])\n",
    "                \n",
    "                total_reward += reward\n",
    "            \n",
    "                ##-----\n",
    "                ## TODO: Q-learning\n",
    "                ##-----\n",
    "                ## find best next action and get corresponding Q-value\n",
    "                q_value = None\n",
    "                target = None\n",
    "                \n",
    "                \n",
    "                ##-----\n",
    "                ## TODO: gradient Q-learning\n",
    "                ##-----\n",
    "                price, past_action = self.env.get_price_action(state)\n",
    "                ## gradient-based parameters update\n",
    "                x_input = np.array([[price, past_action, action]])\n",
    "                y_output = np.array(target).reshape([1,1])\n",
    "                ## one-step optimization\n",
    "\n",
    "                if timestep%10 == 0:\n",
    "                    bar.update(timestep/10 + 1)\n",
    "                    \n",
    "                if terminated:\n",
    "                    break\n",
    "                    \n",
    "                ## for next step\n",
    "                state = next_state\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            bar.finish()\n",
    "            if (e + 1) % 10 == 0:\n",
    "                print(\"Episode: {}\".format(e + 1))\n",
    "                self.display_q()\n",
    "                \n",
    "    def display_q(self):\n",
    "        \"\"\"\n",
    "        heatmap of the Q(s,a) matrix\n",
    "        \"\"\"\n",
    "        Q = np.zeros((self.n_states, self.n_actions))\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_actions):\n",
    "                price, past_action = self.env.get_price_action(i)\n",
    "                Q[i,j] = self.model.predict(np.array([[price, past_action, j]]))\n",
    "        plt.imshow(Q, cmap='hot', interpolation='nearest')\n",
    "        plt.colorbar() \n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "timesteps_per_episode = 250*4\n",
    "env = myTradingEnvV2(timesteps_per_episode)\n",
    "optimizer = Adam(lr=0.001)\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "\n",
    "agent = Agent(env, optimizer, gamma, epsilon)\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display the Q-table approximated by the ANN\n",
    "agent.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_of_episodes = 30\n",
    "rewards = agent.train(num_of_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "The training could be improved by implementing the followings:\n",
    "* batch training\n",
    "* replay episodes/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
