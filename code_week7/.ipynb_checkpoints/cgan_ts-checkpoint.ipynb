{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN for time series generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "## time range\n",
    "start = dt.datetime(2000,1,1)\n",
    "end = dt.datetime(2020,1,1)\n",
    "\n",
    "## download data\n",
    "sp500 = pdr.get_data_yahoo('SPY', start=start, end=end)\n",
    "vix = pdr.get_data_yahoo('^VIX', start=start, end=end)\n",
    "\n",
    "## compute returns\n",
    "returns = np.log(sp500['Close']).diff()\n",
    "\n",
    "## normalize vix\n",
    "vix = vix['Close'] / 100.\n",
    "\n",
    "## display time series\n",
    "plt.figure()\n",
    "returns.plot()\n",
    "plt.title('S&P500 log-returns')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "vix.plot()\n",
    "plt.title('VIX')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_day = 10 ## number of days to simulate\n",
    "\n",
    "n_ts = returns.shape[0]\n",
    "r_vix = vix[1:n_ts-n_day] ## real vix values\n",
    "r_lr = [] ## real returns\n",
    "for i in range(1, n_ts-n_day):\n",
    "    r_lr.append(returns[i:(i+n_day)])\n",
    "r_lr = np.asarray(r_lr)\n",
    "\n",
    "x_train, y_train = r_lr, r_vix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "## inputs = noise\n",
    "## labels = condition (= vix value here)\n",
    "## nday = number of returns to generate\n",
    "def build_generator(inputs, labels, n_day, n_layer=3, h_dim=30):\n",
    "    \n",
    "    ## concatenate inputs\n",
    "    x = None ## TODO\n",
    "    \n",
    "    # ANN layer sizes\n",
    "    hsize = None ## TODO\n",
    "    \n",
    "    ## make layers\n",
    "    ## TODO\n",
    "    \n",
    "    ## output layer\n",
    "    x = None ## TODO\n",
    "    \n",
    "    # input is conditioned by labels\n",
    "    generator = None ## TODO\n",
    "    \n",
    "    return generator\n",
    "\n",
    "def build_discriminator(inputs, labels, n_day, n_layer=3, h_dim=30):\n",
    "    \n",
    "    # ANN layer sizes\n",
    "    hsize = None ## TODO\n",
    "    \n",
    "    ## concatenate inputs\n",
    "    x = None ## TODO\n",
    "    \n",
    "    ## make layers\n",
    "    ## TODO\n",
    "    \n",
    "    ## output layer\n",
    "    x = None ## TODO\n",
    "    \n",
    "    # input is conditioned by labels\n",
    "    generator = None ## TODO\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random labels and noise generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_labels_and_noise(y_train, batch_size, latent_size):\n",
    "    ## bootstrap VIX values + plus uniform noise\n",
    "    fake_vix = np.random.choice(y_train, batch_size) #+ np.random.uniform(-0.05, 0.05, size=batch_size) \n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "    return noise, fake_vix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "## parameters\n",
    "latent_size = 2\n",
    "lr = 1e-4\n",
    "decay = 6e-8\n",
    "input_shape = None ## TODO\n",
    "label_shape = None ## TODO\n",
    "\n",
    "## build discriminator model\n",
    "inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "labels = Input(shape=label_shape, name='class_labels')\n",
    "discriminator = build_discriminator(inputs, labels, n_day)\n",
    "\n",
    "## discriminator converges easily with RMSprop (original paper use Adam)\n",
    "discriminator.compile(loss=None, ## TODO\n",
    "                      optimizer=RMSprop(lr=lr, decay=decay),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "## build generator model\n",
    "input_shape = (latent_size, )\n",
    "inputs = Input(shape=input_shape, name='z_input')\n",
    "generator = build_generator(inputs, labels, n_day)\n",
    "\n",
    "## build adversarial model = generator + discriminator\n",
    "## freeze the weights of discriminator during adversarial training\n",
    "discriminator.trainable = None ## TODO\n",
    "outputs = discriminator([generator([inputs, labels]), labels])\n",
    "combined = Model([inputs, labels],\n",
    "                 outputs, name=\"combined\")\n",
    "combined.compile(loss=None, ## TODO\n",
    "                 optimizer=RMSprop(lr=lr*0.5, decay=decay*0.5),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "## models summaries\n",
    "#discriminator.summary()\n",
    "#generator.summary()\n",
    "combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(x_train, y_train, batch_size):\n",
    "    train_size = x_train.shape[0]\n",
    "    rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "    x_batch = x_train[rand_indexes]\n",
    "    y_batch = y_train[rand_indexes]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "def train(models, data, params):\n",
    "    ## the GAN models made with Keras\n",
    "    generator, discriminator, combined = models\n",
    "    \n",
    "    ## real data\n",
    "    x_train, y_train = data\n",
    "    \n",
    "    ## parameters\n",
    "    batch_size, latent_size, train_steps, nd_steps, ng_steps = params\n",
    "\n",
    "    losses = []\n",
    "    for i in range(int(train_steps/(nd_steps + ng_steps))):\n",
    "        \n",
    "        ## batch dimensions\n",
    "        batch_true = None ## TODO\n",
    "        batch_fake = batch_size - batch_true\n",
    "        \n",
    "        ## randomly select real data\n",
    "        real_ts, real_vix = get_random_batch(x_train, y_train, batch_true)\n",
    "        \n",
    "        ## generate fake data\n",
    "        noise, fake_vix = random_labels_and_noise(y_train, batch_fake, latent_size)\n",
    "        fake_ts = generator.predict([noise, fake_vix])\n",
    "        \n",
    "        ## real + fake = 1 batch\n",
    "        x = np.concatenate((real_ts, fake_ts))\n",
    "        labels = np.concatenate((real_vix, fake_vix))\n",
    "\n",
    "        ## label real == 1.0 and fake == 0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        y[batch_true:, :] = 0.0\n",
    "        \n",
    "        ## train the discriminator\n",
    "        for _ in range(nd_steps):\n",
    "            dloss, dacc = None ## TODO: use train_on_batch\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, dloss, dacc)\n",
    "\n",
    "        ## generate fake data again (could re-use the ones above)\n",
    "        noise, fake_vix = random_labels_and_noise(y_train, batch_size, latent_size)\n",
    "        y = np.ones([batch_size, 1])\n",
    "        \n",
    "        ## train the generator\n",
    "        for _ in range(ng_steps):\n",
    "            gloss, gacc = None ## TODO: use train_on_batch\n",
    "        log = \"%s [generator loss: %f, acc: %f]\" % (log, gloss, gacc)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(log)\n",
    "        \n",
    "        ## save all losses\n",
    "        losses.append([i, i*((nd_steps + ng_steps)), dloss, gloss])\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "batch_size = 256\n",
    "train_steps = int(1e4)\n",
    "nd_steps = 1\n",
    "ng_steps = 1\n",
    "\n",
    "## train discriminator and adversarial networks\n",
    "models = (generator, discriminator, combined)\n",
    "data = (x_train, y_train)\n",
    "params = (batch_size, latent_size, train_steps, nd_steps, ng_steps)\n",
    "losses = train(models, data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(losses, columns=['Iteration', 'Step', 'loss discriminator', 'loss generator'])\n",
    "\n",
    "plt.plot('Step', 'loss discriminator', data=losses)\n",
    "plt.plot('Step', 'loss generator', data=losses)\n",
    "plt.xlabel('Step')\n",
    "plt.legend()\n",
    "plt.title('Training Losses')\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot = 100\n",
    "\n",
    "def plot_ts(noise_input, noise_vix):\n",
    "    gen_ts = generator.predict([noise_input, noise_vix])\n",
    "    plt.figure()\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    for i in range(n_plot):\n",
    "        plt.plot(gen_ts[i])\n",
    "    plt.show()\n",
    "    \n",
    "noise_input = np.random.uniform(-1.0, 1.0, size=[n_plot, latent_size])    \n",
    "\n",
    "plot_ts(noise_input, np.array([0.1] * n_plot))\n",
    "plot_ts(noise_input, np.array([0.9] * n_plot))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pts = 1000\n",
    "\n",
    "x_true, y_true = get_random_batch(x_train, y_train, n_pts)\n",
    "scores_true = discriminator.predict([x_true, y_true])\n",
    "noise_fake, y_fake = random_labels_and_noise(y_train, n_pts, latent_size)\n",
    "scores_fake = combined.predict([noise_fake, y_fake])\n",
    "\n",
    "scores = np.hstack([scores_true, scores_fake])\n",
    "plt.hist(scores)\n",
    "plt.legend(['True', 'Fake'])\n",
    "plt.title('Scores historgram')\n",
    "plt.ylabel('Number of obs.')\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
